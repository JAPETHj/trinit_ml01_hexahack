
# BROWNIES POINTS ADDITIONAL CODE 

# 1) FOR DEPLOYMENTING THE MODEL TO FIREBASE

# THE STEPS THAT WE HAVE FOLLOWED ARE :: 

# Install the TensorFlow Lite converter:
pip install tflite-model-maker

# Load the Kaggle notebook Python file.
python kaggle_notebook.py

# Convert the TensorFlow model to tflite format.
tflite_convert --output_file tflite_model.tflite --saved_model_dir saved_model

# Save the tflite model to a file.
open("tflite_model.tflite", "wb").write(tflite_model)

# Atlast, Python file that you we created repo is converted in to a TensorFlow model to tflite format and saved into the deployment:
import tensorflow as tf

# Load the TensorFlow mode
model = tf.keras.models.load_model('saved_model')

# Convert the TensorFlow model to tflite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the tflite model to a file
open("tflite_model.tflite", "wb").write(tflite_model) 

#2) OUR MODEL APPLICABLE FOR VIDEO DATASETS 

# THE PESDUO CODE OF IT

import cv2
import numpy as np

# Load pre-trained YOLOv3 model
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")

# Load class names
classes = []
with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]

# Initialize video capture object to read video frames
cap = cv2.VideoCapture("video.mp4")

# Define the confidence threshold for object detection
conf_threshold = 0.5

# Define the non-maximum suppression threshold for object detection
nms_threshold = 0.4

# Define the frame dimensions
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Define the output video writer
fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter("output.mp4", fourcc, 30, (frame_width, frame_height))

while True:
    # Capture frame
    ret, frame = cap.read()

    if not ret:
        break

    # Preprocess the frame for YOLOv3 model input
    height, width, channels = frame.shape
    blob = cv2.dnn.blobFromImage(frame, 1 / 255, (416, 416), swapRB=True, crop=False)

    # Run the YOLOv3 model to detect road regions and damage
    net.setInput(blob)
    outs = net.forward(net.getUnconnectedOutLayersNames())

    # Postprocess the detection results (e.g., filter out false positives, combine detections across frames)
    class_ids = []
    confidences = []
    boxes = []

    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]

            if confidence > conf_threshold and classes[class_id] in ["road", "crack", "spalling", "pothole", "rutting", "shoving"]:
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)

                x = int(center_x - w / 2)
                y = int(center_y - h / 2)

                class_ids.append(class_id)
                confidences.append(float(confidence))
                boxes.append([x, y, w, h])

    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)

    # Draw the results on the frame
    for i in indices:
        i = i[0]
        box = boxes[i]
        x, y, w, h = box
        label = f"{classes[class_ids[i]]}: {confidences[i]:.2f}"
        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    # Display the results on the frame
    cv2.imshow("frame", frame)

    # Write the results to the output video
    out.write(frame)

    # Exit if the user presses the 'q' key
    if cv2.waitKey(1) == ord("q"):
        break

# Release video capture object and destroy all windows
cap.release()
out.release()
cv2.destroyAllWindows()
